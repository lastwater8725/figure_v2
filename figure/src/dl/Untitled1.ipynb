{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7vWk-5f4IWQ",
        "outputId": "0e2a8e6b-35bf-4173-f7bb-05bb7aaf8a1b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.metrics import classification_report, roc_curve, auc, precision_recall_curve, average_precision_score, confusion_matrix\n",
        "from sklearn.calibration import calibration_curve\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from itertools import cycle\n",
        "# GPU 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0kJ0vxn4Vrf"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./data/merged_V2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtVG8L_e4cae"
      },
      "outputs": [],
      "source": [
        "# Label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "df['target'] = label_encoder.fit_transform(df['label'])\n",
        "num_classes = len(label_encoder.classes_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtBhPJR8G1oz"
      },
      "outputs": [],
      "source": [
        "feature_cols = [col for col in df.columns if '_x' in col or '_y' in col]\n",
        "print(feature_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hezOOdd_5spt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "\n",
        "# ✅ Feature 컬럼 선택\n",
        "feature_cols = [col for col in df.columns if '_x' in col or '_y' in col]\n",
        "seq_length = 20  # 윈도우 크기\n",
        "\n",
        "# ✅ 1. Pandas → NumPy 변환 (RAM 절약)\n",
        "X_np = df[feature_cols].values.astype(np.float32)  # NumPy 배열로 변환\n",
        "Y_np = df['target'].values.astype(np.int64)\n",
        "\n",
        "del df  # ✅ 원본 데이터프레임 삭제 (메모리 절약)\n",
        "gc.collect()\n",
        "\n",
        "# ✅ 2. 슬라이딩 윈도우 제너레이터 (Colab 안정화)\n",
        "def sliding_window_generator(X, Y, seq_length):\n",
        "    for i in range(len(X) - seq_length):\n",
        "        yield X[i:i+seq_length], Y[i+seq_length]  # ✅ 한 번에 하나씩 반환 (메모리 절약)\n",
        "\n",
        "# ✅ 3. NumPy 배열로 변환 (제너레이터 사용)\n",
        "X_gen = sliding_window_generator(X_np, Y_np, seq_length)\n",
        "\n",
        "# ✅ 4. NumPy 배열 저장 (Colab에서 재사용 가능)\n",
        "X_data = np.array([x for x, _ in X_gen], dtype=np.float32)\n",
        "Y_data = np.array([y for _, y in sliding_window_generator(X_np, Y_np, seq_length)], dtype=np.int64)\n",
        "\n",
        "print(f\"✅ 최종 데이터 크기: X={X_data.shape}, Y={Y_data.shape}\")\n",
        "\n",
        "# ✅ 5. NumPy 파일로 저장 (Colab에서 재사용 가능)\n",
        "np.save(\"X_data.npy\", X_data)\n",
        "np.save(\"Y_data.npy\", Y_data)\n",
        "\n",
        "# ✅ 6. 불필요한 변수 삭제 후 메모리 정리\n",
        "del X_np, Y_np, X_data, Y_data, X_gen\n",
        "gc.collect()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiePxSLF6k7u"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "\n",
        "# ✅ NumPy 데이터 불러오기 (mmap_mode='r' 사용)\n",
        "X = np.load(\"X_data.npy\", mmap_mode=\"r\")  # ✅ 전체를 메모리에 올리지 않음\n",
        "Y = np.load(\"Y_data.npy\", mmap_mode=\"r\")\n",
        "\n",
        "# ✅ 테스트 세트 분리 (한 번만 수행)\n",
        "X_train_full, X_test, Y_train_full, Y_test = train_test_split(\n",
        "    X, Y, test_size=0.2, random_state=42, shuffle=False  # ✅ 시계열 유지\n",
        ")\n",
        "\n",
        "print(f\"✅ Full Train Set: {X_train_full.shape}, Test Set: {X_test.shape}\")\n",
        "\n",
        "# ✅ KFold 설정\n",
        "kf = KFold(n_splits=5, shuffle=False)\n",
        "\n",
        "# ✅ PyTorch Dataset 정의\n",
        "class SkatingDataset(Dataset):\n",
        "    def __init__(self, X, Y, indices):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.indices = indices  # ✅ Fold별 인덱스만 저장하여 RAM 절약\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        real_idx = self.indices[idx]  # ✅ 원본 데이터의 인덱스\n",
        "        return torch.tensor(self.X[real_idx], dtype=torch.float32), torch.tensor(self.Y[real_idx], dtype=torch.long)\n",
        "\n",
        "# ✅ Fold별 데이터 분할\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_full)):\n",
        "    print(f\"\\n=== Fold {fold+1} ===\")\n",
        "\n",
        "    # ✅ Fold별 Dataset 생성 (RAM 절약)\n",
        "    train_dataset = SkatingDataset(X_train_full, Y_train_full, train_idx)\n",
        "    val_dataset = SkatingDataset(X_train_full, Y_train_full, val_idx)\n",
        "    test_dataset = SkatingDataset(X_test, Y_test, np.arange(len(Y_test)))  # 테스트 세트는 전체 사용\n",
        "\n",
        "    # ✅ DataLoader 생성 (배치 크기 조정)\n",
        "    batch_size = 32\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    print(f\"✅ Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
        "    print(\"✅ DataLoader 생성 완료!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIMXNiqnC8-5"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        return self.fc(out[:, -1, :])\n",
        "\n",
        "\n",
        "# class TransformerModel(nn.Module):\n",
        "#     def __init__(self, input_size, hidden_size, output_size, num_heads=4, num_layers=2):\n",
        "#         super().__init__()\n",
        "#         encoder_layer = nn.TransformerEncoderLayer(d_model=input_size, nhead=num_heads)\n",
        "#         self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "#         self.fc = nn.Linear(input_size, output_size)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.transformer_encoder(x)\n",
        "#         return self.fc(x[:, -1, :])\n",
        "\n",
        "input_size = X_train_full.shape[2]  # ✅ Feature 개수\n",
        "hidden_size = 128\n",
        "output_size = num_classes\n",
        "\n",
        "models = {\n",
        "    \"RNN\": RNNModel(input_size, hidden_size, output_size),\n",
        "    #\"Transformer\": TransformerModel(input_size, hidden_size, output_size)\n",
        "}\n",
        "\n",
        "print(\"모델 정의 완료 ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HAcHd0EDFJQ"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import torch.cuda.amp as amp\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_epochs = 10\n",
        "lr = 0.001\n",
        "accumulation_steps = 2\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n--- Training {name} ---\")\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    scaler = amp.GradScaler()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        for step, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            with amp.autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels) / accumulation_steps\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            if (step + 1) % accumulation_steps == 0:\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            train_loss += loss.item() * accumulation_steps\n",
        "            predicted = torch.argmax(outputs, dim=1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_acc = correct / total\n",
        "        print(f\"Epoch {epoch+1}: Train Loss={train_loss / len(train_loader):.4f}, Accuracy={train_acc:.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), f\"{name}_model.pth\")\n",
        "    print(f\"Model {name} saved!\\n\")\n",
        "\n",
        "    del model, optimizer, criterion, scaler\n",
        "    torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaDWuG7oHQVF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "figure",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
